---
title: "Second Amendment Dependencies: Pilot Analysis (Version 3)"
author: "DGK"
date: "Q2 2025"
output:
  html_document: 
    toc: true
    toc_depth: 3
    number_sections: true
    toc_float: true
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Library Setup

```{r}
library(tidyverse)  # for data manipulation
library(lme4)       # for glmer and lmer models
library(ordinal)    # for clmm ordinal logistic regression
library(ggplot2)    # for plotting
library(scales)     # for scales::percent in plots
```

# Initial Formatting

## Set up file paths

```{r}
numeric_file <- "/Users/dgkamper/Library/CloudStorage/GoogleDrive-dgkamper@gmail.com/My Drive/DGK Lab/Collaborations/BlankLang Lab/DGK Lab - BlankLangLab - Second Amendment Language Dependencies/Data/Cleaned_Numeric_Second Amendment Dependencies_Pilot.csv"
response_file <- "/Users/dgkamper/Library/CloudStorage/GoogleDrive-dgkamper@gmail.com/My Drive/DGK Lab/Collaborations/BlankLang Lab/DGK Lab - BlankLangLab - Second Amendment Language Dependencies/Data/Cleaned_Response_Second Amendment Dependencies_Pilot.csv"
```

## Read the numeric file

```{r}
df_numeric <- read_csv(numeric_file)
df_response <- read_csv(response_file)
```
## Reshape data from wide to long format
Each participant saw only one sentence, so we pivot the sentence-specific columns

```{r}
df_long <- df_numeric %>%
  pivot_longer(
    cols = matches("^(Rewrite|Interpretation|Confidence|IsTrue|Remove)[0-9]+$"),
    names_to = c(".value", "sentence"),
    names_pattern = "([A-Za-z]+)([0-9]+)"
  ) %>%
  # Filter to keep only rows where participant actually responded
  filter(!is.na(Rewrite)) %>%
  # Convert sentence identifier to numeric for analysis
  mutate(sentence = as.integer(sentence))

# Save reshaped data for reference
write_csv(df_long, "df_long_cleaned.csv")
```

# Analysis Models

## Participant Exclusion Based on Attention Checks

```{r}
# Apply exclusion criteria and create analysis variables
df_analysis <- df_long %>%
  # Create attention check and comprehension variables
  mutate(
    # Attention check: AttentionCheck1 == 3 is correct
    screen_pass = (AttentionCheck1 == 3),
    # Comprehension checks: Police1 == 2, Police2 == 2, Police3 == 1 are correct
    comp1 = if_else(Police1 == 2, 0, 1),
    comp2 = if_else(Police2 == 2, 0, 1), 
    comp3 = if_else(Police3 == 1, 0, 1),
    comp_total = comp1 + comp2 + comp3
  ) %>%
  # Apply exclusion criteria
  filter(
    screen_pass == TRUE,           # Passed attention check
    comp_total < 2,                # Failed fewer than 2 comprehension checks
    Seriousness == 4               # Reported taking survey seriously
  ) %>%
  # Create analysis variables
  mutate(
    # Binary interpretation variable for logistic regression
    Q2_Interpretation_bin = case_when(
      Interpretation == 1 ~ 0,  # "Only in cases when necessary"
      Interpretation == 2 ~ 1,  # "Always necessary"
      TRUE ~ NA_real_
    ),
    # Factor version for plotting
    Q2_Interpretation_factor = factor(
      Interpretation,
      levels = c(1, 2),
      labels = c("Only in cases when necessary", "Always necessary")
    ),
    # Ordered factor for perceived impact analysis
    Q4_IsTrue_ord = factor(
      IsTrue, 
      ordered = TRUE,
      levels = sort(unique(na.omit(IsTrue))),
      labels = c("Not crucial", "Somewhat important", "Absolutely essential")
    ),
    # Centered removal impact variable (4 = neutral midpoint becomes 0)
    Q5_Remove_centered = as.numeric(Remove) - 4,
    # Ensure confidence is numeric
    Confidence = as.numeric(Confidence)
  )

# Create sentence type classification based on whether sentence refers to "right" or "ability"
sentence_classifications <- tibble(
  sentence = 1:20,
  sentence_type = factor(c(
    "Right",    # 1: archive access
    "Other",    # 2: orchard efforts  
    "Ability",  # 3: fire safety
    "Right",    # 4: tradition preservation
    "Other",    # 5: dam expertise
    "Ability",  # 6: marketplace transactions
    "Ability",  # 7: road construction
    "Ability",  # 8: sundial reading
    "Ability",  # 9: masterpiece creation
    "Right",    # 10: beehive access
    "Ability",  # 11: telescope observation
    "Right",    # 12: shelter access
    "Other",    # 13: food reserve access
    "Other",    # 14: lighthouse guidance
    "Ability",  # 15: garden cultivation
    "Right",    # 16: insulated shelter
    "Right",    # 17: weather station data
    "Right",    # 18: library access
    "Right",    # 19: aqueduct water
    "Ability"   # 20: weathervane reading
  ), levels = c("Right", "Ability", "Other"))
)

# Join sentence classifications with analysis data
df_analysis <- df_analysis %>%
  left_join(sentence_classifications, by = "sentence")

cat("Final sample size after exclusions:", nrow(df_analysis), "participants\n")
cat("Number of sentences:", length(unique(df_analysis$sentence)), "\n")
```

## Question 2: Interpreting “Being Necessary”

Here we assume that the variable Response is a binary indicator (1 = “only in cases when it’s necessary”, 2 = “always necessary”). A logistic mixed-effects model with a random intercept for sentence is used.

```{r}
# Logistic mixed-effects model to test interpretation preferences
model_q2 <- glmer(
  Q2_Interpretation_bin ~ 1 + (1 | sentence),
  data = df_analysis,
  family = binomial
)

summary(model_q2)

# Calculate probability from log-odds for interpretation
intercept_logodds <- fixef(model_q2)[1]
prob_always_necessary <- plogis(intercept_logodds)
cat("\nProbability of choosing 'Always necessary':", round(prob_always_necessary, 3), "\n")
```
## Question 3: Confidence Ratings

This model tests whether confidence ratings differ based on the interpretation (i.e. Interpretation). A linear mixed-effects model is fitted with random slopes and intercepts for sentence.
```{r}
# Linear mixed-effects model testing if confidence differs by interpretation
model_q3 <- lmer(
  Confidence ~ 1 + Q2_Interpretation_bin + (1 + Q2_Interpretation_bin | sentence),
  data = df_analysis
)

summary(model_q3)
```

## Question 4: Regression for Perceived Impact

```{r}
# Ordinal logistic regression for perceived impact
# Try mixed model first, fall back to simpler models if convergence issues
model_q4_mixed <- try(
  clmm(
    Q4_IsTrue_ord ~ Q2_Interpretation_bin + (1 + Q2_Interpretation_bin | sentence),
    data = df_analysis,
    link = "logit"
  ), 
  silent = TRUE
)

if (inherits(model_q4_mixed, "try-error")) {
  cat("Mixed model failed to converge, using simpler models...\n")
  
  # Simple ordinal model without random effects
  model_q4_simple <- clm(
    Q4_IsTrue_ord ~ Q2_Interpretation_bin,
    data = df_analysis,
    link = "logit"
  )
  
  cat("Simple ordinal model results:\n")
  print(summary(model_q4_simple))
  
  # Model with sentence as fixed effect
  model_q4_fixed <- clm(
    Q4_IsTrue_ord ~ Q2_Interpretation_bin + factor(sentence),
    data = df_analysis,
    link = "logit"
  )
  
  cat("\nModel with sentence fixed effects:\n")
  print(summary(model_q4_fixed))
  
} else {
  cat("Mixed model converged successfully:\n")
  print(summary(model_q4_mixed))
}
```
## Question 5: Impact of Clause Removal

For this model, we center the response by subtracting 4 (so that a neutral midpoint equals 0) and then fit a linear mixed-effects model with a random intercept for sentence.

```{r}
# Linear mixed-effects model for clause removal impact
model_q5 <- lmer(
  Q5_Remove_centered ~ 1 + (1 | sentence),
  data = df_analysis
)

summary(model_q5)

# Test if interpretation affects removal impact
model_q5_with_interp <- lmer(
  Q5_Remove_centered ~ 1 + Q2_Interpretation_bin + (1 + Q2_Interpretation_bin | sentence),
  data = df_analysis
)

summary(model_q5_with_interp)
```
# Visualizations

```{r}
# Create a consistent theme for all plots
theme_analysis <- theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 11),
    legend.position = "bottom",
    strip.text = element_text(face = "bold")
  )
```

## Q2: Overall interpretation distribution

```{r}
plot_q2_overall <- df_analysis %>%
  filter(!is.na(Q2_Interpretation_factor)) %>%
  ggplot(aes(x = Q2_Interpretation_factor)) +
  geom_bar(aes(y = after_stat(count)/sum(after_stat(count))), 
           fill = "steelblue", alpha = 0.7, width = 0.6) +
  geom_text(
    aes(y = after_stat(count)/sum(after_stat(count)), 
        label = percent(after_stat(count)/sum(after_stat(count)), accuracy = 0.1)),
    stat = "count", vjust = -0.5, size = 4
  ) +
  scale_y_continuous(
    labels = percent_format(accuracy = 1),
    limits = c(0, 1),
    breaks = seq(0, 1, 0.2)
  ) +
  labs(
    title = "Overall Distribution of Second Amendment Structure Interpretations",
    subtitle = "How participants interpret 'being necessary' clauses",
    x = "Interpretation Type",
    y = "Proportion of Participants"
  ) +
  theme_analysis

print(plot_q2_overall)
```

## Q2: Interpretation by sentence type

```{r}
plot_q2_by_type <- df_analysis %>%
  filter(!is.na(Q2_Interpretation_factor), !is.na(sentence_type)) %>%
  count(sentence_type, Q2_Interpretation_factor) %>%
  group_by(sentence_type) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot(aes(x = Q2_Interpretation_factor, y = prop, fill = Q2_Interpretation_factor)) +
  geom_col(alpha = 0.7, width = 0.6) +
  geom_text(
    aes(label = percent(prop, accuracy = 0.1)), 
    vjust = -0.5, size = 3.5
  ) +
  scale_y_continuous(
    labels = percent_format(accuracy = 1),
    limits = c(0, 1),
    breaks = seq(0, 1, 0.25)
  ) +
  scale_fill_manual(
    values = c("coral", "steelblue"),
    name = "Interpretation"
  ) +
  facet_wrap(~sentence_type) +
  labs(
    title = "Interpretation Patterns by Sentence Type",
    subtitle = "Comparing 'Rights' vs 'Abilities' vs 'Other' sentence constructions",
    x = "Interpretation Type",
    y = "Proportion within Sentence Type"
  ) +
  theme_analysis +
  theme(axis.text.x = element_text(angle = 15, hjust = 1))

print(plot_q2_by_type)
```
## Q3: Confidence distribution

```{r}
plot_q3_overall <- df_analysis %>%
  filter(!is.na(Confidence)) %>%
  ggplot(aes(x = Confidence)) +
  geom_histogram(
    aes(y = after_stat(density)), 
    binwidth = 1, fill = "steelblue", alpha = 0.7, color = "white"
  ) +
  geom_density(alpha = 0.3, fill = "steelblue") +
  scale_x_continuous(breaks = 1:7, limits = c(0.5, 7.5)) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  labs(
    title = "Distribution of Confidence in Interpretation",
    subtitle = "How confident participants felt about their chosen interpretation",
    x = "Confidence Rating (1 = Not at all confident, 7 = Extremely confident)",
    y = "Density"
  ) +
  theme_analysis

print(plot_q3_overall)
```
## Q3: Confidence by interpretation and sentence type

```{r}
plot_q3_by_type <- df_analysis %>%
  filter(!is.na(Confidence), !is.na(Q2_Interpretation_factor), !is.na(sentence_type)) %>%
  ggplot(aes(x = Q2_Interpretation_factor, y = Confidence, fill = sentence_type)) +
  geom_boxplot(alpha = 0.7, outlier.alpha = 0.6) +
  scale_fill_manual(
    values = c("coral", "steelblue", "darkgreen"),
    name = "Sentence Type"
  ) +
  facet_wrap(~sentence_type) +
  labs(
    title = "Confidence by Interpretation Type and Sentence Category",
    subtitle = "Comparing confidence levels across different sentence constructions",
    x = "Interpretation Type",
    y = "Confidence Rating (1-7)"
  ) +
  theme_analysis +
  theme(
    axis.text.x = element_text(angle = 15, hjust = 1),
    legend.position = "none"  # Remove legend since facets show the same info
  )

print(plot_q3_by_type)
```
## Q4: Perceived impact distribution

```{r}
plot_q4_overall <- df_analysis %>%
  filter(!is.na(Q4_IsTrue_ord)) %>%
  ggplot(aes(x = Q4_IsTrue_ord)) +
  geom_bar(aes(y = after_stat(count)/sum(after_stat(count))), 
           fill = "darkgreen", alpha = 0.7, width = 0.6) +
  geom_text(
    aes(y = after_stat(count)/sum(after_stat(count)), 
        label = percent(after_stat(count)/sum(after_stat(count)), accuracy = 0.1)),
    stat = "count", vjust = -0.5, size = 4
  ) +
  scale_y_continuous(
    labels = percent_format(accuracy = 1),
    limits = c(0, 0.6),
    breaks = seq(0, 0.6, 0.1)
  ) +
  labs(
    title = "Perceived Impact of Prefatory Clause",
    subtitle = "How important participants view the introductory 'being necessary' clause",
    x = "Perceived Importance of Prefatory Clause",
    y = "Proportion of Participants"
  ) +
  theme_analysis +
  theme(axis.text.x = element_text(angle = 20, hjust = 1))

print(plot_q4_overall)
```
```{r}
# Q4: Perceived impact by sentence type
plot_q4_by_type <- df_analysis %>%
  filter(!is.na(Q4_IsTrue_ord), !is.na(sentence_type)) %>%
  count(sentence_type, Q4_IsTrue_ord) %>%
  group_by(sentence_type) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot(aes(x = Q4_IsTrue_ord, y = prop, fill = Q4_IsTrue_ord)) +
  geom_col(alpha = 0.7, width = 0.6) +
  geom_text(
    aes(label = percent(prop, accuracy = 0.1)), 
    vjust = -0.5, size = 3
  ) +
  scale_y_continuous(
    labels = percent_format(accuracy = 1),
    limits = c(0, 0.7),
    breaks = seq(0, 0.7, 0.1)
  ) +
  scale_fill_manual(
    values = c("coral", "steelblue", "darkgreen"),
    name = "Perceived Impact"
  ) +
  facet_wrap(~sentence_type) +
  labs(
    title = "Perceived Impact of Prefatory Clause by Sentence Type",
    subtitle = "How sentence content affects perceived importance of introductory clause",
    x = "Perceived Importance Level",
    y = "Proportion within Sentence Type"
  ) +
  theme_analysis +
  theme(axis.text.x = element_text(angle = 20, hjust = 1))

print(plot_q4_by_type)
```
## Q5: Impact of clause removal

```{r}
plot_q5_overall <- df_analysis %>%
  filter(!is.na(Q5_Remove_centered)) %>%
  ggplot(aes(x = Q5_Remove_centered)) +
  geom_histogram(
    aes(y = after_stat(density)), 
    binwidth = 1, fill = "steelblue", alpha = 0.7, color = "white"
  ) +
  geom_density(alpha = 0.3, fill = "steelblue") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red", size = 1) +
  scale_x_continuous(breaks = -3:3) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  labs(
    title = "Impact of Removing Prefatory Clause",
    subtitle = "How much meaning changes when 'being necessary' clause is removed (0 = no change)",
    x = "Centered Response (-3 = Much less similar, 0 = No change, +3 = Much more similar)",
    y = "Density"
  ) +
  theme_analysis

print(plot_q5_overall)
```

## Q5: Removal impact by sentence type

```{r}
plot_q5_by_type <- df_analysis %>%
  filter(!is.na(Q5_Remove_centered), !is.na(sentence_type)) %>%
  ggplot(aes(x = sentence_type, y = Q5_Remove_centered, fill = sentence_type)) +
  geom_boxplot(alpha = 0.7, outlier.alpha = 0.6) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", size = 1) +
  scale_fill_manual(
    values = c("coral", "steelblue", "darkgreen"),
    name = "Sentence Type"
  ) +
  labs(
    title = "Impact of Clause Removal by Sentence Type",
    subtitle = "Comparing how sentence content affects perceived change when clause is removed",
    x = "Sentence Type",
    y = "Centered Response (Impact of Removal)"
  ) +
  theme_analysis +
  theme(legend.position = "none")

print(plot_q5_by_type)
```
# Summary Statistics

```{r}
# Create summary table
summary_stats <- df_analysis %>%
  summarise(
    n_participants = n(),
    n_sentences = n_distinct(sentence),
    prop_always_necessary = mean(Q2_Interpretation_bin, na.rm = TRUE),
    mean_confidence = mean(Confidence, na.rm = TRUE),
    sd_confidence = sd(Confidence, na.rm = TRUE),
    mean_removal_impact = mean(Q5_Remove_centered, na.rm = TRUE),
    sd_removal_impact = sd(Q5_Remove_centered, na.rm = TRUE)
  )

print(summary_stats, width = Inf)

# Summary by sentence type
summary_by_type <- df_analysis %>%
  group_by(sentence_type) %>%
  summarise(
    n = n(),
    prop_always_necessary = mean(Q2_Interpretation_bin, na.rm = TRUE),
    mean_confidence = mean(Confidence, na.rm = TRUE),
    mean_removal_impact = mean(Q5_Remove_centered, na.rm = TRUE),
    .groups = "drop"
  )

print(summary_by_type)
```
```{r}
# Export cleaned data and summary for further analysis
write_csv(df_analysis, "second_amendment_analysis_cleaned.csv")
write_csv(summary_by_type, "summary_by_sentence_type.csv")

# Export Q1 rewrite responses for qualitative coding
q1_responses <- df_analysis %>%
  select(ResponseId, Prolific, sentence, sentence_type, Rewrite, Q2_Interpretation_factor) %>%
  arrange(sentence)

write_csv(q1_responses, "q1_rewrite_responses_for_coding.csv")
```

# Open Ended Response Analysis

```{r}
# Read the coded qualitative data
coded_data <- read_csv("q1_rewrite_responses_for_coding_edited.csv", show_col_types = FALSE)

# Data Structure
glimpse(coded_data)

# Check the coding categories in the Analysis column
table(coded_data$Analysis, useNA = "ifany")

# Check the distribution of explicit interpretation choices
table(coded_data$Q2_Interpretation_factor, useNA = "ifany")
```

## Clean and prepare data for analysis

```{r}
analysis_data <- coded_data %>%
  # Remove any rows with missing values in key variables
  filter(!is.na(Analysis), !is.na(Q2_Interpretation_factor)) %>%
  # Standardize the Analysis coding for consistency
  mutate(
    # Create a cleaned version of the Analysis variable
    Analysis_Clean = case_when(
      Analysis == "Always" ~ "Always",
      Analysis == "Not Always" ~ "Not Always", 
      Analysis == "Ambiguous" ~ "Ambiguous",
      TRUE ~ Analysis  # Keep any other values as-is
    ),
    # Create binary versions for easier statistical analysis
    Explicit_Always = if_else(Q2_Interpretation_factor == "Always necessary", 1, 0),
    Coded_Always = if_else(Analysis_Clean == "Always", 1, 0),
    # Calculate agreement between explicit choice and coded rewrite
    Agreement = if_else(
      (Q2_Interpretation_factor == "Always necessary" & Analysis_Clean == "Always") |
      (Q2_Interpretation_factor == "Only in cases when necessary" & Analysis_Clean == "Not Always"),
      "Agreement", "Disagreement"
    )
  )

cat("Total responses:", nrow(analysis_data), "\n")
cat("Responses by sentence type:\n")
print(table(analysis_data$sentence_type))
```
## Concordance Analysis: Explicit vs. Implicit

```{r}
# Create contingency table for explicit choice vs. coded rewrite
concordance_table <- table(
  Explicit = analysis_data$Q2_Interpretation_factor,
  Coded = analysis_data$Analysis_Clean
)

print("Contingency Table: Explicit Choice vs. Coded Rewrite")
print(concordance_table)

# Calculate percentages within each explicit choice category
concordance_props <- prop.table(concordance_table, margin = 1) * 100
print("\nPercentages within each explicit choice:")
print(round(concordance_props, 1))

# Calculate overall agreement rate (excluding ambiguous cases for now)
agreement_data <- analysis_data %>%
  filter(Analysis_Clean != "Ambiguous")

agreement_rate <- mean(agreement_data$Agreement == "Agreement", na.rm = TRUE)
cat("\nOverall Agreement Rate (excluding ambiguous):", round(agreement_rate * 100, 1), "%\n")

# Test for association using Chi-square test
if(min(concordance_table) >= 5) {  # Check if Chi-square assumptions are met
  chi_test <- chisq.test(concordance_table)
  cat("Chi-square test p-value:", format.pval(chi_test$p.value), "\n")
  
  # Calculate Cramer's V for effect size
  cramers_v <- sqrt(chi_test$statistic / (sum(concordance_table) * (min(dim(concordance_table)) - 1)))
  cat("Cramer's V (effect size):", round(cramers_v, 3), "\n")
} else {
  cat("Note: Chi-square test assumptions not met (expected cell counts < 5)\n")
}
```
## Analysis by Sentence Type

```{r}
# Create breakdown by sentence type
sentence_analysis <- analysis_data %>%
  group_by(sentence_type) %>%
  summarise(
    n = n(),
    # Explicit choices
    explicit_always_pct = mean(Q2_Interpretation_factor == "Always necessary") * 100,
    # Coded rewrites
    coded_always_pct = mean(Analysis_Clean == "Always", na.rm = TRUE) * 100,
    coded_not_always_pct = mean(Analysis_Clean == "Not Always", na.rm = TRUE) * 100,
    coded_ambiguous_pct = mean(Analysis_Clean == "Ambiguous", na.rm = TRUE) * 100,
    # Agreement rate (excluding ambiguous)
    agreement_rate = mean(Agreement == "Agreement", na.rm = TRUE) * 100,
    .groups = "drop"
  )

print("Analysis by Sentence Type:")
print(sentence_analysis, width = Inf)
```
## Visualizations
```{r}
# Plot 1: Overall Concordance Heatmap
concordance_plot_data <- as.data.frame(concordance_table) %>%
  mutate(
    Percentage = round(Freq / sum(Freq) * 100, 1),
    Label = paste0(Freq, "\n(", Percentage, "%)")
  )

plot_concordance <- ggplot(concordance_plot_data, aes(x = Coded, y = Explicit)) +
  geom_tile(aes(fill = Freq), alpha = 0.8) +
  geom_text(aes(label = Label), size = 4, fontface = "bold") +
  scale_fill_gradient(low = "grey", high = "steelblue", name = "Count") +
  labs(
    title = "Concordance Between Explicit Choice and Coded Rewrite",
    subtitle = "Numbers show count and percentage of total responses",
    x = "Coded Analysis of Rewrite",
    y = "Explicit Multiple Choice Response"
  ) +
  theme_analysis +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(plot_concordance)

# Plot 2: Agreement Rates by Sentence Type
agreement_by_type <- analysis_data %>%
  filter(Analysis_Clean != "Ambiguous") %>%
  group_by(sentence_type) %>%
  summarise(
    n = n(),
    agreement_rate = mean(Agreement == "Agreement") * 100,
    .groups = "drop"
  )

plot_agreement <- ggplot(agreement_by_type, aes(x = sentence_type, y = agreement_rate)) +
  geom_col(fill = "darkgreen", alpha = 0.7, width = 0.6) +
  geom_text(aes(label = paste0(round(agreement_rate, 1), "%\n(n=", n, ")")), 
            vjust = -0.5, size = 4, fontface = "bold") +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, 20),
                     labels = function(x) paste0(x, "%")) +
  labs(
    title = "Agreement Between Explicit Choice and Coded Rewrite",
    subtitle = "Percentage of cases where multiple choice and rewrite coding align",
    x = "Sentence Type",
    y = "Agreement Rate"
  ) +
  theme_analysis

print(plot_agreement)

# Plot 3: Detailed Breakdown by Sentence Type
detailed_data <- analysis_data %>%
  count(sentence_type, Q2_Interpretation_factor, Analysis_Clean) %>%
  group_by(sentence_type, Q2_Interpretation_factor) %>%
  mutate(
    total_in_group = sum(n),
    percentage = n / total_in_group * 100
  ) %>%
  ungroup()

plot_detailed <- ggplot(detailed_data, 
                       aes(x = Q2_Interpretation_factor, y = percentage, fill = Analysis_Clean)) +
  geom_col(position = "stack", alpha = 0.8) +
  geom_text(aes(label = ifelse(percentage > 8, paste0(round(percentage, 0), "%"), "")),
            position = position_stack(vjust = 0.5), size = 3, fontface = "bold") +
  scale_fill_manual(
    values = c("Always" = "steelblue", "Not Always" = "coral", "Ambiguous" = "gray60"),
    name = "Coded Analysis"
  ) +
  scale_y_continuous(labels = function(x) paste0(x, "%")) +
  facet_wrap(~sentence_type) +
  labs(
    title = "Coded Analysis Distribution by Explicit Choice and Sentence Type",
    subtitle = "How rewrite coding varies within each explicit interpretation choice",
    x = "Explicit Multiple Choice Response",
    y = "Percentage within Choice Category"
  ) +
  theme_analysis +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(plot_detailed)

# Plot 4: Disagreement Pattern Analysis
disagreement_data <- analysis_data %>%
  filter(Agreement == "Disagreement", Analysis_Clean != "Ambiguous") %>%
  mutate(
    Disagreement_Type = case_when(
      Q2_Interpretation_factor == "Always necessary" & Analysis_Clean == "Not Always" ~ 
        "Said 'Always' but wrote 'Conditional'",
      Q2_Interpretation_factor == "Only in cases when necessary" & Analysis_Clean == "Always" ~ 
        "Said 'Conditional' but wrote 'Always'",
      TRUE ~ "Other"
    )
  ) %>%
  count(sentence_type, Disagreement_Type)

if(nrow(disagreement_data) > 0) {
  plot_disagreements <- ggplot(disagreement_data, 
                              aes(x = sentence_type, y = n, fill = Disagreement_Type)) +
    geom_col(position = "stack", alpha = 0.8) +
    geom_text(aes(label = n), position = position_stack(vjust = 0.5), 
              size = 4, fontface = "bold") +
    scale_fill_manual(
      values = c("Said 'Always' but wrote 'Conditional'" = "coral",
                 "Said 'Conditional' but wrote 'Always'" = "steelblue"),
      name = "Type of Disagreement"
    ) +
    labs(
      title = "Patterns of Disagreement Between Explicit Choice and Coded Rewrite",
      subtitle = "Where participants' conscious choices don't match their intuitive language",
      x = "Sentence Type",
      y = "Number of Disagreements"
    ) +
    theme_analysis

  print(plot_disagreements)
}
```
## Summary Statistics

```{r}
# Calculate key statistics
total_responses <- nrow(analysis_data)
non_ambiguous <- sum(analysis_data$Analysis_Clean != "Ambiguous")
agreements <- sum(analysis_data$Agreement == "Agreement", na.rm = TRUE)
disagreements <- sum(analysis_data$Agreement == "Disagreement", na.rm = TRUE)

cat("Total coded responses:", total_responses, "\n")
cat("Non-ambiguous responses:", non_ambiguous, "\n")
cat("Perfect agreements:", agreements, "\n")
cat("Disagreements:", disagreements, "\n")
cat("Overall agreement rate:", round(agreements / non_ambiguous * 100, 1), "%\n")

# Analyze disagreement patterns
disagreement_summary <- analysis_data %>%
  filter(Agreement == "Disagreement", Analysis_Clean != "Ambiguous") %>%
  mutate(
    Disagreement_Direction = case_when(
      Q2_Interpretation_factor == "Always necessary" & Analysis_Clean == "Not Always" ~ 
        "Explicit_Always_Coded_Conditional",
      Q2_Interpretation_factor == "Only in cases when necessary" & Analysis_Clean == "Always" ~ 
        "Explicit_Conditional_Coded_Always"
    )
  ) %>%
  count(Disagreement_Direction)

if(nrow(disagreement_summary) > 0) {
  cat("\nDisagreement patterns:\n")
  print(disagreement_summary)
}

# Create final summary for export
final_summary <- list(
  overall_stats = list(
    total_responses = total_responses,
    agreement_rate = round(agreements / non_ambiguous * 100, 1),
    ambiguous_rate = round(sum(analysis_data$Analysis_Clean == "Ambiguous") / total_responses * 100, 1)
  ),
  by_sentence_type = sentence_analysis,
  concordance_table = concordance_table,
  disagreement_patterns = disagreement_summary
)
```

